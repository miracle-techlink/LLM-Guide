<h1 align="center">å¤§æ¨¡å‹2.0 ä¸‹ä¸€ä»£é«˜æ•ˆæ¨ç†èŒƒå¼ LLM-Guide</h1>

<p align="center"> </p>


> LLM-guide,æ˜¯ä¸ªäººçš„å­¦ä¹ ç¬”è®°ï¼Œå¸Œæœ›å¸®åŠ©åˆ°æ›´å¤šäººï¼Œæ¶µç›–äº†tuning,RLHF,ç¡¬ä»¶ä¼˜åŒ–ï¼Œåº•å±‚æ¶æ„ä¼˜åŒ–ï¼Œinfraå¤šç»´åº¦çš„å†…å®¹, æ¬¢è¿ç‚¹Starã€åˆ†äº«ä¸æPRğŸŒŸ~<br>ã€ <a href="https://github.com/miracle-techlink/LLM-Guide">LLM-Guide</a>, Latest Update: April, 2025 ã€‘

### LLMåˆ†å¸ƒå¼è®­ç»ƒ
**1.åŸºç¡€æ¦‚å¿µ**
- **æŠ€æœ¯è§£è¯»:** [LLMåˆ†å¸ƒå¼è®­ç»ƒ---åŸºç¡€æ¦‚å¿µ](https://miracle-techlink.github.io/posts/9aac52d9.html)

**2.æ•°æ®å¹¶è¡Œ**
- **æŠ€æœ¯è§£è¯»ï¼š**[LLMåˆ†å¸ƒå¼è®­ç»ƒ---æ•°æ®å¹¶è¡Œ](https://miracle-techlink.github.io/posts/3a50363.html)


 **[â¬† ä¸€é”®è¿”å›ç›®å½•](#ç›®å½•)**

### LLMåº•å±‚æ¶æ„

æœ¬ä»“åº“åŒ…å«ä¸å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åº•å±‚æ¶æ„ç›¸å…³çš„è®ºæ–‡åŠæŠ€æœ¯è§£è¯»æ–‡æ¡£ï¼ŒåŒ…æ‹¬ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ã€çº¿æ€§æ³¨æ„åŠ›ä»¥åŠå¤šæ¨¡æ€å¤„ç†ç­‰æŠ€æœ¯ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›é‡è¦è®ºæ–‡çš„åˆ—è¡¨åŠå…¶é“¾æ¥ï¼Œä¾›è¿›ä¸€æ­¥é˜…è¯»ã€‚

**1. Native Sparse Attention (NSA)**
- **è®ºæ–‡:** [Native Sparse Attention](https://arxiv.org/abs/2206.09768)
- **æŠ€æœ¯è§£è¯»:** [ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶](https://link_to_tech_doc.com)
- **è§£è¯»:** ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶åœ¨å‡å°‘è®¡ç®—å¤æ‚åº¦çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿åºåˆ—æ•°æ®ï¼Œå‡å°‘å†…å­˜æ¶ˆè€—ã€‚

**2. Hyena Hierarchy**
- **è®ºæ–‡:** [Hyena: A New Sparse Attention Architecture](https://arxiv.org/abs/2206.09645)
- **æŠ€æœ¯è§£è¯»:** [Hyenaæ¶æ„è§£æ](https://link_to_tech_doc.com)
- **è§£è¯»:** Hyenaæ¶æ„æ˜¯ä¸€ç§æ–°çš„ç¨€ç–æ³¨æ„åŠ›æ¶æ„ï¼Œèƒ½å¤Ÿå¤„ç†æ›´é•¿èŒƒå›´çš„è¾“å…¥æ•°æ®ï¼Œæå‡æ¨¡å‹æ¨ç†é€Ÿåº¦ã€‚

**3. Perceiver / Perceiver IO**
- **è®ºæ–‡:** [Perceiver: Generalized Attention for Multi-modal Data](https://arxiv.org/abs/2103.03206)
- **æŠ€æœ¯è§£è¯»:** [Perceiver IOè§£æ](https://link_to_tech_doc.com)
- **è§£è¯»:** Perceiveræ¶æ„èƒ½å¤Ÿå¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘ï¼‰ï¼Œåœ¨é«˜æ•ˆçš„è®¡ç®—å’Œå¤šä»»åŠ¡å­¦ä¹ æ–¹é¢æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚

**4. Mixture-of-Transformers (MoT)**
- **è®ºæ–‡:** [Mixture of Transformers: Sparse Multi-modal Processing](https://arxiv.org/abs/2103.10332)
- **æŠ€æœ¯è§£è¯»:** [Mixture of Transformersè§£æ](https://link_to_tech_doc.com)
- **è§£è¯»:** MoTæ¶æ„ç»“åˆäº†ç¨€ç–æ³¨æ„åŠ›å’Œå¤šæ¨¡æ€å¤„ç†ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡å¤šä»»åŠ¡å­¦ä¹ çš„æ•ˆç‡ã€‚

**5. FlashAttention**
- **è®ºæ–‡:** [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.10614)
- **è§£è¯»:** è¯¥æ¶æ„ä¼˜åŒ–äº†ä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæå‡äº†å†…å­˜æ•ˆç‡å¹¶åŠ é€Ÿäº†è®¡ç®—ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦é«˜æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†æ—¶ã€‚

**6. çº¿æ€§æ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰**
- **è®ºæ–‡:** [Linear Transformers are Secretly Fast Weight Programmers](https://arxiv.org/abs/2006.04768)
- **è§£è¯»:** è¿™ç¯‡è®ºæ–‡æå‡ºé€šè¿‡çº¿æ€§æ³¨æ„åŠ›æ¥è§£å†³ä¼ ç»ŸTransformeråœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„è®¡ç®—ç“¶é¢ˆï¼Œæé«˜äº†å¤„ç†é€Ÿåº¦å’Œæ•ˆç‡ã€‚

**7. ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰**
- **è®ºæ–‡:** [H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](https://arxiv.org/abs/2207.05308)
- **è®ºæ–‡:** [MoBA: Mixture of Block Attention for Long-Context LLMs](https://arxiv.org/abs/2207.07744)
- **è®ºæ–‡:** [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2209.10899)

**8. çº¿æ€§æ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰**
- **è®ºæ–‡:** [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2206.07152)
- **è®ºæ–‡:** [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/abs/2209.01377)
- **è®ºæ–‡:** [Gated Delta Networks: Improving Mamba2 with Delta Rule](https://arxiv.org/abs/2209.08535)
  
**[â¬† ä¸€é”®è¿”å›ç›®å½•](#ç›®å½•)**
---
